## Ordinary Least Squares Regression

The ordinary least squares (OLS) regression algorithm is a generative, supervised machine learning algorithm which attempts to model data by fitting it to a straight line. The algorithm takes coordinates in the form ``` (x_i, y_i)``` as its input, and for each coordinate it calculates a straight line, ```\hat{y_i} = mx_i+c``` where m (the gradient of the line), and c (the y-intercept) are both chosen arbitrarily. It then calculates the residual, ```r_i``` of each point, where ```r_i = \hat{y_i}-y_i```. How well the straight line fits the data can then be represented by an error in the form ```\sum_{\forall_i} r_i^2```, with a lower value representing coordinates which are closer to the line, thereby being a better fit. The algorithm then repeats this process, each time choosing different values of the gradient and y-intercept. Once the algorithm has found values which minimise the error, it uses that straight line as its final model, outputting the values of m and c that it has used. 

Once the algorithm has generated a model, it can be used to generate new data. If the training data that was used is well described by a straight line, the generated data will look like the training data, making the model useful to predict future data.

As a very basic model, OLS is of limited scientific use, since data that has been deemed suitable for analysis by machine learning is unlikely to be well described by a simple straight line fit. However the simplicity of the maths involved does mean that it can be a good algorithm to use to learn the basics of machine learning.
