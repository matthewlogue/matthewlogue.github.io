## Ordinary Least Squares Regression

The ordinary least squares (OLS) regression algorithm is a generative, supervised machine learning algorithm which attempts to model data by fitting it to a straight line. The algorithm takes coordinates in the form ![x sub i, y sub i](https://latex.codecogs.com/svg.latex?\small&space;(x_i,&space;y_i)) as its input, and for each coordinate it calculates a straight line, ![y_hat = mx + c](https://latex.codecogs.com/svg.latex?\small&space;\hat{y_i}&space;=&space;mx_i&plus;c) where m (the gradient of the line), and c (the y-intercept) are both chosen arbitrarily. It then calculates the residual, ![ri](https://latex.codecogs.com/svg.latex?\small&space;r_i) of each point, where ![y diff](https://latex.codecogs.com/svg.latex?\small&space;r_i&space;=&space;\hat{y_i}-y_i). How well the straight line fits the data can then be represented by an error in the form ![sum](https://latex.codecogs.com/svg.latex?\small&space;\sum_{\forall_i}&space;r_i^2), with a lower value representing coordinates which are closer to the line, thereby being a better fit. The algorithm then repeats this process, each time choosing different values of the gradient and y-intercept. Once the algorithm has found values which minimise the error, it uses that straight line as its final model, outputting the values of m and c that it has used. 

Once the algorithm has generated a model, it can be used to generate new data. If the training data that was used is well described by a straight line, the generated data will look like the training data, making the model useful to predict future data.

As a very basic model, OLS is of limited scientific use, since data that has been deemed suitable for analysis by machine learning is unlikely to be well described by a simple straight line fit. However the simplicity of the maths involved does mean that it can be a good algorithm to use to learn the basics of machine learning.
